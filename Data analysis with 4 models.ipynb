{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00890f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note :Real represent Propoganda and fake represents non propoganda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3663bbea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import xticks\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Embedding,Activation,Dropout\n",
    "from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d55f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "train = dt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12131de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd15adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw bar plot\n",
    "def draw_bar_plot(category,length,xlabel,ylabel,title,sub):\n",
    "    plt.subplot(2,2,sub)\n",
    "    plt.bar(category, length)\n",
    "    plt.legend()\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    plt.title(title, fontsize=15)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw histogram\n",
    "def draw_hist(xlabel, ylabel,title,target,sub,color):\n",
    "    plt.subplot(1,2,sub)\n",
    "    plt.hist(train[train.target==target][\"length\"],color = color)\n",
    "    plt.title(title,fontsize=25)\n",
    "    plt.xlabel(xlabel,fontsize=15)\n",
    "    plt.ylabel(ylabel,fontsize=15)\n",
    "    plt.ylim(0,1200)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw graphs for stopwords and punctuations\n",
    "def draw_bar_n_plot(data,title):\n",
    "# lets visualize the top 10 stop words\n",
    "    x,y=zip(*data)\n",
    "\n",
    "    plt.figure(figsize = (25,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(x,y)\n",
    "    plt.title(\"Top 10 \"+ title,fontsize=25)\n",
    "    plt.xlabel(title,fontsize=15)\n",
    "    plt.ylabel(\"Count\",fontsize=15)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(x,y,'g')\n",
    "    plt.title(\"Top 10 \"+ title,fontsize=25)\n",
    "    plt.xlabel(title,fontsize=15)\n",
    "    plt.ylabel(\"Count\",fontsize=15)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of propoganda (Target = 1):\",len(train[train[\"target\"]==1]))\n",
    "print(\"No. of not propoganda (Target = 0):\",len(train[train[\"target\"]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"target\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c328194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(data):    \n",
    "    return len(data)\n",
    "\n",
    "train[\"length\"]= train.data.apply(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2262df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "draw_hist(\"Real Propoganda Data\",\"Length of Data\",\"Length of Real Propoganda Data\",1, 1,\"darkgreen\")\n",
    "draw_hist(\"Fake Not propoganda Data\",\"Length of Data\",\"Length of Fake not Propoganda Data\",0, 2,\"darkred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59610b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.groupby(\"target\").mean()[\"length\"].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ef161",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(\"length\",1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1080e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e64dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename(columns = {'data':'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b40040",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4868cda8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sw \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mtext:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stop:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "sw = []\n",
    "for message in train.text:\n",
    "    for word in message.split():\n",
    "        if word in stop:\n",
    "            sw.append(word)\n",
    "\n",
    "\n",
    "# lets convert the list to a dictinoary which would contain the stop words and their frequency\n",
    "wordlist = nltk.FreqDist(sw)\n",
    "# lets save the 10 most frequent stopwords\n",
    "top10 = wordlist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d82a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bar_n_plot(top10,\"Stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pun = []\n",
    "for message in train.text:\n",
    "    for word in message.split():\n",
    "        if word in punctuation:\n",
    "            pun.append(word)\n",
    "\n",
    "\n",
    "wordlist = nltk.FreqDist(pun)\n",
    "# lets save the 10 most frequent stopwords\n",
    "top10 = wordlist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bar_n_plot(top10,\"Punctuations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_real = []\n",
    "pun_real  = []\n",
    "for message in train[train.target==1][\"text\"]:\n",
    "    for word in message.split():\n",
    "        if word in stop:\n",
    "            stop_real.append(word)\n",
    "        if word in punctuation:\n",
    "            pun_real.append(word)\n",
    "\n",
    "\n",
    "stop_real_wordlist = nltk.FreqDist(stop_real)\n",
    "pun_real_wordlist =  nltk.FreqDist(pun_real)\n",
    "\n",
    "# lets save the 10 most frequent stopwords\n",
    "stop_real_top10 = stop_real_wordlist.most_common(10)\n",
    "pun_real_top10  = pun_real_wordlist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_fake = []\n",
    "pun_fake  = []\n",
    "for message in train[train.target==0][\"text\"]:\n",
    "    for word in message.split():\n",
    "        if word in stop:\n",
    "            stop_fake.append(word)\n",
    "        if word in punctuation:\n",
    "            pun_fake.append(word)\n",
    "\n",
    "\n",
    "# lets convert the list to a dictinoary which would contain the stop word and its frequency\n",
    "stop_fake_wordlist = nltk.FreqDist(stop_fake)\n",
    "pun_fake_wordlist =  nltk.FreqDist(pun_fake)\n",
    "\n",
    "# lets save the 10 most frequent stopwords\n",
    "stop_fake_top10 = stop_fake_wordlist.most_common(10)\n",
    "pun_fake_top10  = pun_fake_wordlist.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stop_real,y_stop_real=zip(*stop_real_top10)\n",
    "x_pun_real, y_pun_real =zip(*pun_real_top10)\n",
    "\n",
    "x_stop_fake,y_stop_fake=zip(*stop_fake_top10)\n",
    "x_pun_fake, y_pun_fake=zip(*pun_fake_top10)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (30,30))\n",
    "draw_bar_plot(x_stop_real,y_stop_real,\"Stopwords\",\"Count\",\"Top 10 Stopwords - propoganda\",1)\n",
    "draw_bar_plot(x_stop_fake,y_stop_fake,\"Stopwords\",\"Count\",\"Top 10 Stopwords - Non Propoganda\",2)\n",
    "draw_bar_plot(x_pun_real,y_pun_real,\"Punctuations\",\"Count\",\"Top 10 Punctuations - propoganda\",3)\n",
    "draw_bar_plot(x_pun_fake,y_pun_fake,\"Punctuations\",\"Count\",\"Top 10 Punctuations - Non propoganda\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_pun = stop + punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the messages\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text) # removing urls \n",
    "    text = re.sub('[^\\w]',' ',text)          \n",
    "    text = re.sub('[\\d]','',text) # this will remove numeric characters\n",
    "    text = text.lower()\n",
    "    words = text.split()  \n",
    "    sentence = \"\"\n",
    "    for word in words:     \n",
    "        if word not in (sw_pun):  # removing stopwords & punctuations                \n",
    "            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n",
    "            if len(word) > 3: # we will consider words with length  greater than 3 only\n",
    "                sentence = sentence + word + ' '             \n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50682c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing functions on the train and test datasets\n",
    "train['text'] = train['text'].apply(lambda s : preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdba53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function on the train and the test datasets\n",
    "train['text'] = train['text'].apply(lambda s : remove_emoji(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create vocab\n",
    "from collections import Counter\n",
    "def create_vocab(df):\n",
    "    vocab = Counter()\n",
    "    for i in range(df.shape[0]):\n",
    "        vocab.update(df.text[i].split())\n",
    "    return(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263077f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate training and testing datasets\n",
    "#master=pd.concat((train,test)).reset_index(drop=True)\n",
    "\n",
    "# call vocabulary creation function on master dataset\n",
    "vocab = create_vocab(train)\n",
    "\n",
    "# lets check the no. of words in the vocabulary\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ef04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final vocab by considering words with more than one occurence\n",
    "final_vocab = []\n",
    "min_occur = 2\n",
    "for k,v in vocab.items():\n",
    "    if v >= min_occur:\n",
    "        final_vocab.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3318b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter the dataset, keep only words which are present in the vocab\n",
    "def filter(text):\n",
    "    sentence = \"\"\n",
    "    for word in text.split():  \n",
    "        if word in final_vocab:\n",
    "            sentence = sentence + word + ' '\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a78aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda s : filter(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "propoganda = train[train.target==1].reset_index()\n",
    "not_propoganda = train[train.target==0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create top 100 n-grams\n",
    "def get_ngrams(data,n):\n",
    "    all_words = []\n",
    "    for i in range(len(data)):\n",
    "        temp = data[\"text\"][i].split()\n",
    "        for word in temp:\n",
    "            all_words.append(word)\n",
    "\n",
    "    tokenized = all_words\n",
    "    esBigrams = ngrams(tokenized, n)\n",
    "\n",
    "    esBigram_wordlist = nltk.FreqDist(esBigrams)\n",
    "    top100 = esBigram_wordlist.most_common(100)\n",
    "    top100 = dict(top100)\n",
    "    df_ngrams = pd.DataFrame(sorted(top100.items(), key=lambda x: x[1])[::-1])\n",
    "    return df_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a560b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_barplots(real,fake,title):\n",
    "    plt.figure(figsize = (40,80),dpi=100)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.barplot(y=real[0].values[:100], x=real[1].values[:100], color='green')\n",
    "    plt.title(\"Top 100\" + title + \"in Real Propoganda\",fontsize=15)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sns.barplot(y=fake[0].values[:100], x=fake[1].values[:100],color='red')\n",
    "    plt.title(\"Top 100\" + title + \"in Non Propoganda\",fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9da19b",
   "metadata": {},
   "source": [
    "# unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create top 100 unigrams\n",
    "real_unigrams = get_ngrams(propoganda,1)\n",
    "fake_unigrams = get_ngrams(not_propoganda,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a889df",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_barplots(real_unigrams,fake_unigrams,\" Unigrams \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf486746",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create top 100 bigrams\n",
    "real_bigrams = get_ngrams(propoganda,2)\n",
    "fake_bigrams = get_ngrams(not_propoganda,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5777c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_barplots(real_bigrams,fake_bigrams,\" Bigrams \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716b849",
   "metadata": {},
   "source": [
    "# Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create top 100 trigrams\n",
    "real_trigrams = get_ngrams(propoganda,3)\n",
    "fake_trigrams = get_ngrams(not_propoganda,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765927b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_barplots(real_trigrams,fake_trigrams,\" Trigrams \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(df):\n",
    "    comment_words = '' \n",
    "    stopwords = set(STOPWORDS) \n",
    "\n",
    "    # iterate through the csv file \n",
    "    for val in df.text: \n",
    "\n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "        \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower()\n",
    "        \n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "        #return comment_words\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "            background_color ='white', \n",
    "            stopwords = stopwords, \n",
    "            min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud(propoganda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead15684",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud(not_propoganda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ad13c",
   "metadata": {},
   "source": [
    "# Model Building & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate f1 score for each epoch\n",
    "import keras.backend as K\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4af870",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.text\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ceb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and apply tokenizer on the training dataset\n",
    "tokenizer = create_tokenizer(X_train)\n",
    "X_train_set = tokenizer.texts_to_matrix(X_train, mode = 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_f1])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "n_words = X_train_set.shape[1]\n",
    "model = define_model(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_set,y_train,epochs=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4692f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on the test dataset\n",
    "X_test_set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\n",
    "y_pred = model.predict(X_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c16f80",
   "metadata": {},
   "source": [
    "# ANN with KERAS Word Embeddings¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db366c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a tokenizer on text will create a list of unique words with an integer assigned to it\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(X_train.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8734f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72727b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100,  input_length=100, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[get_f1])\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# fit the model\n",
    "model.fit(padded_docs, y_train, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bb8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80cc29",
   "metadata": {},
   "source": [
    "# Lstm with word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6589957",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(512, embed_dim,input_length = X_train_set.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='RMSprop')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "hist = model.fit(X_train_set,y_train,epochs = 57, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91848f65",
   "metadata": {},
   "source": [
    "# Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add tokens to the data make it BERT compatible\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ce425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03255057",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(m_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b895c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993056e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951890c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
